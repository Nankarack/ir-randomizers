{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "The image is build using the following command:\n",
    "\n",
    "```\n",
    "docker build -t registry.webis.de/code-research/tira/tira-user-ir-lab-sose-2023-randomizers/iranthology:0.0.1 .\n",
    "```\n",
    "\n",
    "This docker image also contains jupyter, so that we can run/test this notebook via the following docker command:\n",
    "\n",
    "```\n",
    "docker run -p 8888:8888 --rm -ti -w /workspace -v ${PWD}:/workspace --entrypoint jupyter registry.webis.de/code-research/tira/tira-user-ir-lab-sose-2023-randomizers/iranthology:0.0.1 notebook --allow-root --ip 0.0.0.0\n",
    "```\n",
    "\n",
    "For the build image, the following two test commands work:\n",
    "\n",
    "First, exporting the data:\n",
    "\n",
    "```\n",
    "tira-run \\\n",
    "    --output-directory ${PWD}/iranthology-dataset-tira \\\n",
    "    --image registry.webis.de/code-research/tira/tira-user-ir-lab-sose-2023-randomizers/iranthology:0.0.1 \\\n",
    "    --allow-network true \\\n",
    "    --command '/irds_cli.sh --ir_datasets_id iranthology-randomizers --output_dataset_path $outputDir'\n",
    "```\n",
    "\n",
    "Second, producing a ranking with an existing retrieval model:\n",
    "\n",
    "```\n",
    "tira-run \\\n",
    "    --input-directory ${PWD}/iranthology-dataset-tira \\\n",
    "    --image webis/tira-ir-starter-pyterrier:0.0.1-base \\\n",
    "    --command '/workspace/run-pyterrier-notebook.py --input $inputDataset --output $outputDir --notebook /workspace/full-rank-pipeline.ipynb'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our data integration\n",
    "\n",
    "The ir_datasets integration is in the file `iranthology.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ir_datasets\r\n",
      "from ir_datasets.formats import JsonlDocs, TrecXmlQueries\r\n",
      "from typing import NamedTuple\r\n",
      "from ir_datasets.datasets.base import Dataset\r\n",
      "\r\n",
      "class IrAnthologyDocument(NamedTuple):\r\n",
      "    doc_id: str\r\n",
      "    text: str\r\n",
      "    \r\n",
      "    def default_text(self):\r\n",
      "        return self.text\r\n",
      "\r\n",
      "ir_datasets.registry.register('iranthology-randomizers', Dataset(\r\n",
      "    JsonlDocs(ir_datasets.util.PackageDataFile(path='datasets_in_progress/ir-anthology-processed.jsonl'), doc_cls=IrAnthologyDocument, lang='en'),\r\n",
      "    TrecXmlQueries(ir_datasets.util.PackageDataFile(path='datasets_in_progress/topics.xml'), lang='en')\r\n",
      "))\r\n"
     ]
    }
   ],
   "source": [
    "!cat iranthology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for reflection cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IrAnthologyDocument(doc_id='2014.cikm_workshop-2014dtmbio.20', text=\"{'crossref': 'DBLP:conf/cikm/2014dtmbio', 'booktitle': 'Proceedings of the ACM 8th International Workshop on Data and Text Mining in Bioinformatics, DTMBIO@CIKM 2014, Shanghai, China, November 7, 2014', 'pages': '47', 'publisher': 'ACM', 'year': '2014', 'url': 'https://doi.org/10.1145/2665970.2665989', 'doi': '10.1145/2665970.2665989', 'biburl': 'https://dblp.org/rec/conf/cikm/JungYYKCKL14.bib', 'bibsource': 'dblp computer science bibliography, https://dblp.org', 'bibkey': 'DBLP:conf/cikm/JungYYKCKL14', 'bibtype': 'inproceedings', 'authors': ['Jinmyung Jung', 'Hasun Yu', 'Seyeol Yoon', 'Mijin Kwon', 'Sungji Choo', 'Sangwoo Kim', 'Doheon Lee'], 'editors': [], 'venue': 'CIKM', 'id': '2014.cikm_workshop-2014dtmbio.20', 'date': 1541519853.0, 'abstract': 'ABSTRACTInferring drug-induced phenotypes via computational approaches can give a substantial support to drug discovery procedure. However, existing computational models that are mainly based on a single cell or a single organ model are thought to be limited because the phenotypes are consequences of stochastic biochemical processes among distant cells/organs as well as molecules confined in one cell. Therefore, there is an urgent demand for a new computational model that represents heterogeneous biochemical interactions spanning the entire human body. To meet the demand, we constructed multi-level networks that incorporate previously uncovered high-level properties such as molecules, cells, organs, and phenotypes. Currently, the networks consist of 1,776,506 edges including molecular networks within 76 pre-defined cell-types, inter-cell interactions among the cell-types, and gene (protein) relations to 429 phenotypes. We are also planning to verify if known druginduced phenotypes are reproducible in the networks using a Petri-net based simulation.', 'title': 'Construction of Multi-level Networks Incorporating Molecule, Cell, Organ and Phenotype Properties for Drug-induced Phenotype Prediction'}\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"iranthology-randomizers\")\n",
    "\n",
    "example_document = dataset.docs_store().get('2014.cikm_workshop-2014dtmbio.20')\n",
    "example_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe the document above and maybe the intention behind design decisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:Information Retrieval with algorithms\n",
      "2:misspellings in queries\n",
      "3:information in different language\n",
      "4:Abbreviations in queries\n",
      "5:lemmatization algorithms\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"iranthology-randomizers\")\n",
    "\n",
    "for query in dataset.queries_iter():\n",
    "    print(query.query_id +':' + query.title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe something for queries above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1 (Group: Randomizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./ir-anthology-07-11-2021-ss23.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "lis = []\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    id = result[\"id\"]\n",
    "    lis.append({\"doc_id\" : id, \"text\" : f\"{result}\"})\n",
    "    #  the contents of \"text\" are subject to change\n",
    "\n",
    "with open(\"ir-anthology-processed.jsonl\", 'w') as f:\n",
    "    for item in lis:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSONL processing\n",
    "This first cell reads the given .jsonl file and processes it into a new one of the desired form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from ir_datasets.formats import JsonlDocs, TrecXmlQueries\n",
    "from typing import NamedTuple\n",
    "from ir_datasets.datasets.base import Dataset\n",
    "\n",
    "class IrAnthologyDocument(NamedTuple):\n",
    "    doc_id: str\n",
    "    text: str\n",
    "    \n",
    "    def default_text(self):\n",
    "        return self.text\n",
    "\n",
    "ir_datasets.registry.register('iranthology-randomizers', Dataset(\n",
    "    JsonlDocs(ir_datasets.util.PackageDataFile(path='datasets_in_progress/ir-anthology-processed.jsonl'), doc_cls=IrAnthologyDocument, lang='en'),\n",
    "    TrecXmlQueries(ir_datasets.util.PackageDataFile(path='datasets_in_progress/topics.xml'), lang='en')\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registry\n",
    "\n",
    "This cell registers the dataset into ir-datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "Working in a group of 5 can be messy, however we managed to organize our group quite well. Finding a day and time at which everyone is free was not easy, but we figured it out, got to know each other a little and decided on how to approach the task.\n",
    "\n",
    "Working on the script to process the .jsonl file was simple, with some prior python experience and some good documentation and examples to be found on the internet.\n",
    "More difficult was figuring out how to apply all the steps from the pangram example in the tutorial to the actual asignment. Alot of things, like the registration script and the tira-run command were just given as examples and figuring out how to 'translate' these examples to fit our files was not easy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
